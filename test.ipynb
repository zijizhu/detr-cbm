{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "import copy\n",
    "import math\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nested import as_nested_tensor\n",
    "from torchvision.datasets import CocoDetection\n",
    "from pycocotools.coco import COCO\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from model import build_model\n",
    "from detr.models.detr import PostProcess, SetCriterion\n",
    "from detr.models.matcher import build_matcher\n",
    "import detr.util.misc as utils\n",
    "from cocoeval import CocoEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetrClipDataset(Dataset):\n",
    "    def __init__(self, dataset_dir, coco_dir, split) -> None:\n",
    "        self.data = []\n",
    "        assert split in ['train', 'val']\n",
    "        ann_filename = 'instances_train2017.json' if split == 'train' else 'instances_val2017.json'\n",
    "        self.coco = COCO(os.path.join(coco_dir, 'annotations', ann_filename))\n",
    "        print('Loading preprocessed samples into memory...')\n",
    "        sample_filenames = [fn for fn in os.listdir(dataset_dir) if f'detr_clip_{split}' in fn]\n",
    "        for fn in tqdm(sample_filenames):\n",
    "            self.data += torch.load(os.path.join(dataset_dir, fn))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        detr_f, clip_f = item['detr_f'], item['clip_f']\n",
    "        outputs, targets = item['outputs'],item['targets']\n",
    "        detr_logits, detr_boxes = outputs['pred_logits'], outputs['pred_boxes']\n",
    "        return detr_f, clip_f.float(), detr_logits, detr_boxes, targets\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = list(zip(*batch))\n",
    "    batch = [torch.stack(item) for item in batch[:-1]] + [batch[-1]]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=9.23s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading preprocessed samples into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:30<00:00,  1.33s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset_train = DetrClipDataset('data', 'coco', 'train')\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=8, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train_iter = iter(dataloader_train)\n",
    "detr_f, clip_f, detr_logits, detr_boxes, targets = next(dataloader_train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 100, 256]),\n",
       " torch.Size([8, 512]),\n",
       " torch.Size([8, 100, 92]),\n",
       " torch.Size([8, 100, 4]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detr_f.size(), clip_f.size(), detr_logits.size(), detr_boxes.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detr_f.dtype, clip_f.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 100, 92])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self) -> None:\n",
    "        self.nhead = 8\n",
    "        self.num_layers = 6\n",
    "        self.dim_feedforward = 2048\n",
    "        self.dropout = 0.1\n",
    "        self.set_cost_class = 1\n",
    "        self.set_cost_bbox = 5\n",
    "        self.set_cost_giou = 2\n",
    "        self.bbox_loss_coef = 5\n",
    "        self.giou_loss_coef = 2\n",
    "        self.eos_coef = 0.1\n",
    "        self.lr = 1e-4\n",
    "        self.weight_decay = 1e-4\n",
    "        self.lr_drop = 200\n",
    "\n",
    "args = Args()\n",
    "model = build_model(256, 512, torch.randn(92, 512), args)\n",
    "\n",
    "out = model(clip_f, detr_f)\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr,\n",
    "                                  weight_decay=args.weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)\n",
    "\n",
    "weight_dict = {'loss_ce': 1, 'loss_bbox': args.bbox_loss_coef, 'loss_giou': args.giou_loss_coef}\n",
    "losses = ['labels', 'boxes', 'cardinality']\n",
    "\n",
    "matcher = build_matcher(args)\n",
    "criterion = SetCriterion(91, matcher, weight_dict, args.eos_coef, losses=losses)\n",
    "postprocessors = PostProcess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged stats: lr: 0.000100  class_error: 98.33  loss: 48.7010 (48.7010)  loss_ce: 47.8404 (47.8404)  loss_bbox: 0.1203 (0.1203)  loss_giou: 0.7403 (0.7403)  loss_ce_unscaled: 47.8404 (47.8404)  class_error_unscaled: 98.3333 (98.3333)  loss_bbox_unscaled: 0.0241 (0.0241)  loss_giou_unscaled: 0.3702 (0.3702)  cardinality_error_unscaled: 92.5000 (92.5000)\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "max_norm = 0\n",
    "device = 'mps'\n",
    "\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "\n",
    "model.train()\n",
    "criterion.train()\n",
    "metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "metric_logger.add_meter('class_error', utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
    "header = 'Epoch: [{}]'.format(epoch)\n",
    "print_freq = 10\n",
    "\n",
    "for detr_fs, clip_fs, detr_logits, detr_boxes, targets in metric_logger.log_every(dataloader_train, print_freq, header):\n",
    "    detr_fs, clip_fs = detr_fs.to(device), clip_fs.to(device)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    logits = model(clip_fs, detr_fs)\n",
    "    outputs = {'pred_logits': logits, 'pred_boxes': detr_boxes.to(device)}\n",
    "    loss_dict = criterion(outputs, targets)\n",
    "    weight_dict = criterion.weight_dict\n",
    "    losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "\n",
    "    # reduce losses over all GPUs for logging purposes\n",
    "    loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "    loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
    "                                    for k, v in loss_dict_reduced.items()}\n",
    "    loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
    "                                for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
    "    losses_reduced_scaled = sum(loss_dict_reduced_scaled.values())\n",
    "\n",
    "    loss_value = losses_reduced_scaled.item()\n",
    "\n",
    "    # if not math.isfinite(loss_value):\n",
    "    #     print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "    #     print(loss_dict_reduced)\n",
    "    #     sys.exit(1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    losses.backward()\n",
    "    if max_norm > 0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "    optimizer.step()\n",
    "\n",
    "    metric_logger.update(loss=loss_value, **loss_dict_reduced_scaled, **loss_dict_reduced_unscaled)\n",
    "    metric_logger.update(class_error=loss_dict_reduced['class_error'])\n",
    "    metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "    break\n",
    "# gather the stats from all processes\n",
    "metric_logger.synchronize_between_processes()\n",
    "print(\"Averaged stats:\", metric_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.82s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading preprocessed samples into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.08s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset_val = DetrClipDataset('data', 'coco', 'val')\n",
    "dataloader_val = DataLoader(dataset_train, batch_size=8, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  [    0/14375]  eta: 2:21:00  class_error: 100.00  loss: 53.3654 (53.3654)  loss_ce: 52.7156 (52.7156)  loss_bbox: 0.1563 (0.1563)  loss_giou: 0.4935 (0.4935)  loss_ce_unscaled: 52.7156 (52.7156)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.0313 (0.0313)  loss_giou_unscaled: 0.2467 (0.2467)  cardinality_error_unscaled: 9.7500 (9.7500)  time: 0.5886  data: 0.0026\n",
      "Test:  [   10/14375]  eta: 0:52:20  class_error: 100.00  loss: 47.5468 (44.6227)  loss_ce: 46.9116 (44.0275)  loss_bbox: 0.1425 (0.1423)  loss_giou: 0.4828 (0.4529)  loss_ce_unscaled: 46.9116 (44.0275)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.0285 (0.0285)  loss_giou_unscaled: 0.2414 (0.2264)  cardinality_error_unscaled: 7.8750 (7.4205)  time: 0.2186  data: 0.0024\n",
      "Test:  [   20/14375]  eta: 0:45:05  class_error: 100.00  loss: 43.5932 (43.8926)  loss_ce: 43.1739 (43.3393)  loss_bbox: 0.1349 (0.1342)  loss_giou: 0.4130 (0.4192)  loss_ce_unscaled: 43.1739 (43.3393)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.0270 (0.0268)  loss_giou_unscaled: 0.2065 (0.2096)  cardinality_error_unscaled: 6.7500 (7.1726)  time: 0.1685  data: 0.0022\n",
      "Test:  [   30/14375]  eta: 0:42:21  class_error: 100.00  loss: 40.1465 (43.4798)  loss_ce: 39.6711 (42.9338)  loss_bbox: 0.1222 (0.1343)  loss_giou: 0.3668 (0.4116)  loss_ce_unscaled: 39.6711 (42.9338)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.0244 (0.0269)  loss_giou_unscaled: 0.1834 (0.2058)  cardinality_error_unscaled: 5.8750 (6.9677)  time: 0.1543  data: 0.0018\n",
      "Test:  [   40/14375]  eta: 0:41:48  class_error: 100.00  loss: 40.1607 (43.1433)  loss_ce: 39.6889 (42.5960)  loss_bbox: 0.1307 (0.1352)  loss_giou: 0.3686 (0.4121)  loss_ce_unscaled: 39.6889 (42.5960)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.0261 (0.0270)  loss_giou_unscaled: 0.1843 (0.2060)  cardinality_error_unscaled: 6.0000 (6.9055)  time: 0.1609  data: 0.0019\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m     res \u001b[38;5;241m=\u001b[39m {target[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem(): output \u001b[38;5;28;01mfor\u001b[39;00m target, output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(targets, results)}\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m coco_evaluator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m         \u001b[43mcoco_evaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# gather the stats from all processes\u001b[39;00m\n\u001b[1;32m     50\u001b[0m metric_logger\u001b[38;5;241m.\u001b[39msynchronize_between_processes()\n",
      "File \u001b[0;32m~/Developer/Research/detr-cbm/cocoeval.py:46\u001b[0m, in \u001b[0;36mCocoEvaluator.update\u001b[0;34m(self, predictions)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mdevnull, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m devnull:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mredirect_stdout(devnull):\n\u001b[0;32m---> 46\u001b[0m         coco_dt \u001b[38;5;241m=\u001b[39m \u001b[43mCOCO\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadRes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoco_gt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;28;01melse\u001b[39;00m COCO()\n\u001b[1;32m     47\u001b[0m coco_eval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoco_eval[iou_type]\n\u001b[1;32m     49\u001b[0m coco_eval\u001b[38;5;241m.\u001b[39mcocoDt \u001b[38;5;241m=\u001b[39m coco_dt\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/logichoi/lib/python3.11/site-packages/pycocotools/coco.py:366\u001b[0m, in \u001b[0;36mCOCO.loadRes\u001b[0;34m(self, resFile)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDONE (t=\u001b[39m\u001b[38;5;132;01m{:0.2f}\u001b[39;00m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39m tic))\n\u001b[1;32m    365\u001b[0m res\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotations\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m anns\n\u001b[0;32m--> 366\u001b[0m \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/logichoi/lib/python3.11/site-packages/pycocotools/coco.py:100\u001b[0m, in \u001b[0;36mCOCO.createIndex\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset:\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m--> 100\u001b[0m         imgs[img[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m img\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset:\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cat \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_loader = dataloader_val\n",
    "base_ds = dataset_train.coco\n",
    "device = 'mps'\n",
    "\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    criterion.eval()\n",
    "\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('class_error', utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
    "    header = 'Test:'\n",
    "\n",
    "    iou_types = ('bbox',)\n",
    "    coco_evaluator = CocoEvaluator(base_ds, iou_types)\n",
    "\n",
    "    panoptic_evaluator = None\n",
    "\n",
    "    for detr_fs, clip_fs, detr_logits, detr_boxes, targets in metric_logger.log_every(data_loader, 10, header):\n",
    "        detr_fs, clip_fs = detr_fs.to(device), clip_fs.to(device)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        logits = model(clip_fs, detr_fs)\n",
    "        outputs = {'pred_logits': logits, 'pred_boxes': detr_boxes.to(device)}\n",
    "\n",
    "        loss_dict = criterion(outputs, targets)\n",
    "        weight_dict = criterion.weight_dict\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
    "                                    for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
    "        loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
    "                                      for k, v in loss_dict_reduced.items()}\n",
    "        metric_logger.update(loss=sum(loss_dict_reduced_scaled.values()),\n",
    "                             **loss_dict_reduced_scaled,\n",
    "                             **loss_dict_reduced_unscaled)\n",
    "        metric_logger.update(class_error=loss_dict_reduced['class_error'])\n",
    "\n",
    "        orig_target_sizes = torch.stack([t[\"orig_size\"] for t in targets], dim=0)\n",
    "        results = postprocessors(outputs, orig_target_sizes)\n",
    "        \n",
    "        res = {target['image_id'].item(): output for target, output in zip(targets, results)}\n",
    "        if coco_evaluator is not None:\n",
    "            coco_evaluator.update(res)\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    if coco_evaluator is not None:\n",
    "        coco_evaluator.synchronize_between_processes()\n",
    "\n",
    "    # accumulate predictions from all images\n",
    "    if coco_evaluator is not None:\n",
    "        coco_evaluator.accumulate()\n",
    "        coco_evaluator.summarize()\n",
    "    stats = {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
    "    if coco_evaluator is not None:\n",
    "        stats['coco_eval_bbox'] = coco_evaluator.coco_eval['bbox'].stats.tolist()\n",
    "    # return stats, coco_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logichoi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
