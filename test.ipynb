{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "import copy\n",
    "import math\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nested import as_nested_tensor\n",
    "from torchvision.datasets import CocoDetection\n",
    "from pycocotools.coco import COCO\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from model import build_model\n",
    "from detr.models.detr import PostProcess, SetCriterion\n",
    "from detr.models.matcher import build_matcher\n",
    "import detr.util.misc as utils\n",
    "from cocoeval import CocoEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetrClipDataset(Dataset):\n",
    "    def __init__(self, dataset_dir, coco_dir, split) -> None:\n",
    "        self.data = []\n",
    "        assert split in ['train', 'val']\n",
    "        ann_filename = 'instances_train2017.json' if split == 'train' else 'instances_val2017.json'\n",
    "        self.coco = COCO(os.path.join(coco_dir, 'annotations', ann_filename))\n",
    "        print('Loading preprocessed samples into memory...')\n",
    "        sample_filenames = [fn for fn in os.listdir(dataset_dir) if f'detr_clip_{split}' in fn]\n",
    "        for fn in tqdm(sample_filenames):\n",
    "            self.data += torch.load(os.path.join(dataset_dir, fn))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        detr_f, clip_f = item['detr_f'], item['clip_f']\n",
    "        outputs, targets = item['outputs'],item['targets']\n",
    "        detr_logits, detr_boxes = outputs['pred_logits'], outputs['pred_boxes']\n",
    "        return detr_f, clip_f.float(), detr_logits, detr_boxes, targets\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = list(zip(*batch))\n",
    "    batch = [torch.stack(item) for item in batch[:-1]] + [batch[-1]]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = DetrClipDataset('data', 'coco', 'train')\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=8, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train_iter = iter(dataloader_train)\n",
    "detr_f, clip_f, detr_logits, detr_boxes, targets = next(dataloader_train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detr_f.size(), clip_f.size(), detr_logits.size(), detr_boxes.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detr_f.dtype, clip_f.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self) -> None:\n",
    "        self.nhead = 8\n",
    "        self.num_layers = 6\n",
    "        self.dim_feedforward = 2048\n",
    "        self.dropout = 0.1\n",
    "        self.set_cost_class = 1\n",
    "        self.set_cost_bbox = 5\n",
    "        self.set_cost_giou = 2\n",
    "        self.bbox_loss_coef = 5\n",
    "        self.giou_loss_coef = 2\n",
    "        self.eos_coef = 0.1\n",
    "        self.lr = 1e-4\n",
    "        self.weight_decay = 1e-4\n",
    "        self.lr_drop = 200\n",
    "\n",
    "args = Args()\n",
    "model = build_model(256, 512, torch.randn(92, 512), args)\n",
    "\n",
    "out = model(clip_f, detr_f)\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr,\n",
    "                                  weight_decay=args.weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)\n",
    "\n",
    "weight_dict = {'loss_ce': 1, 'loss_bbox': args.bbox_loss_coef, 'loss_giou': args.giou_loss_coef}\n",
    "losses = ['labels', 'boxes', 'cardinality']\n",
    "\n",
    "matcher = build_matcher(args)\n",
    "criterion = SetCriterion(91, matcher, weight_dict, args.eos_coef, losses=losses)\n",
    "postprocessors = PostProcess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "max_norm = 0\n",
    "device = 'mps'\n",
    "\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "\n",
    "model.train()\n",
    "criterion.train()\n",
    "metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "metric_logger.add_meter('class_error', utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
    "header = 'Epoch: [{}]'.format(epoch)\n",
    "print_freq = 10\n",
    "\n",
    "for detr_fs, clip_fs, detr_logits, detr_boxes, targets in metric_logger.log_every(dataloader_train, print_freq, header):\n",
    "    detr_fs, clip_fs = detr_fs.to(device), clip_fs.to(device)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    logits = model(clip_fs, detr_fs)\n",
    "    outputs = {'pred_logits': logits, 'pred_boxes': detr_boxes.to(device)}\n",
    "    loss_dict = criterion(outputs, targets)\n",
    "    weight_dict = criterion.weight_dict\n",
    "    losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "\n",
    "    # reduce losses over all GPUs for logging purposes\n",
    "    loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "    loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
    "                                    for k, v in loss_dict_reduced.items()}\n",
    "    loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
    "                                for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
    "    losses_reduced_scaled = sum(loss_dict_reduced_scaled.values())\n",
    "\n",
    "    loss_value = losses_reduced_scaled.item()\n",
    "\n",
    "    # if not math.isfinite(loss_value):\n",
    "    #     print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "    #     print(loss_dict_reduced)\n",
    "    #     sys.exit(1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    losses.backward()\n",
    "    if max_norm > 0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "    optimizer.step()\n",
    "\n",
    "    metric_logger.update(loss=loss_value, **loss_dict_reduced_scaled, **loss_dict_reduced_unscaled)\n",
    "    metric_logger.update(class_error=loss_dict_reduced['class_error'])\n",
    "    metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "    break\n",
    "# gather the stats from all processes\n",
    "metric_logger.synchronize_between_processes()\n",
    "print(\"Averaged stats:\", metric_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val = DetrClipDataset('data', 'coco', 'val')\n",
    "dataloader_val = DataLoader(dataset_train, batch_size=8, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = dataloader_val\n",
    "base_ds = dataset_train.coco\n",
    "device = 'mps'\n",
    "\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    criterion.eval()\n",
    "\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('class_error', utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
    "    header = 'Test:'\n",
    "\n",
    "    iou_types = ('bbox',)\n",
    "    coco_evaluator = CocoEvaluator(base_ds, iou_types)\n",
    "\n",
    "    panoptic_evaluator = None\n",
    "\n",
    "    for detr_fs, clip_fs, detr_logits, detr_boxes, targets in metric_logger.log_every(data_loader, 10, header):\n",
    "        detr_fs, clip_fs = detr_fs.to(device), clip_fs.to(device)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        logits = model(clip_fs, detr_fs)\n",
    "        outputs = {'pred_logits': logits, 'pred_boxes': detr_boxes.to(device)}\n",
    "\n",
    "        loss_dict = criterion(outputs, targets)\n",
    "        weight_dict = criterion.weight_dict\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
    "                                    for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
    "        loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
    "                                      for k, v in loss_dict_reduced.items()}\n",
    "        metric_logger.update(loss=sum(loss_dict_reduced_scaled.values()),\n",
    "                             **loss_dict_reduced_scaled,\n",
    "                             **loss_dict_reduced_unscaled)\n",
    "        metric_logger.update(class_error=loss_dict_reduced['class_error'])\n",
    "\n",
    "        orig_target_sizes = torch.stack([t[\"orig_size\"] for t in targets], dim=0)\n",
    "        results = postprocessors(outputs, orig_target_sizes)\n",
    "        \n",
    "        res = {target['image_id'].item(): output for target, output in zip(targets, results)}\n",
    "        if coco_evaluator is not None:\n",
    "            coco_evaluator.update(res)\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    if coco_evaluator is not None:\n",
    "        coco_evaluator.synchronize_between_processes()\n",
    "\n",
    "    # accumulate predictions from all images\n",
    "    if coco_evaluator is not None:\n",
    "        coco_evaluator.accumulate()\n",
    "        coco_evaluator.summarize()\n",
    "    stats = {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
    "    if coco_evaluator is not None:\n",
    "        stats['coco_eval_bbox'] = coco_evaluator.coco_eval['bbox'].stats.tolist()\n",
    "    # return stats, coco_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logichoi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
